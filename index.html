<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Burak Varıcı</title> <meta name="author" content="Burak Varıcı"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bvarici.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Varici_CV.pdf" target="_blank" rel="noopener noreferrer">cv <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Burak Varıcı </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/varici2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/varici2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/varici2021-1400.webp"></source> <img src="/assets/img/varici2021.jpeg?ba5b59e99af0c3cb3395b7f8894994e5" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="varici2021.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> <div style="text-align:center;"> <p style="font-size:1.15rem; font-weight:600;">Burak Varıcı</p> <br> <p>Postdoc, CMU</p> <p><a href="mailto:bvarici@andrew.cmu.edu">bvarici@andrew.cmu.edu</a></p> </div> </div> </div> <div class="clearfix"> <p>I am a postdoctoral researcher in the Machine Learning Department at <a href="https://www.ml.cmu.edu/" rel="external nofollow noopener" target="_blank">CMU</a>, working with <a href="https://www.cs.cmu.edu/~pradeepr/" rel="external nofollow noopener" target="_blank">Pradeep Ravikumar</a>. Previously, I obtained my Ph.D. degree in Electrical, Computer and Systems Engineering (ECSE), Rensselaer Polytechnic Institute (<a href="https://www.ecse.rpi.edu/" rel="external nofollow noopener" target="_blank">RPI</a>), advised by <a href="https://www.isg-rpi.com/" rel="external nofollow noopener" target="_blank">Ali Tajer</a>. My research was supported by an <a href="https://fcrc.rpi.edu/" rel="external nofollow noopener" target="_blank">IBM AI Horizons PhD Fellowship</a>.</p> <p>I broadly work at the intersection of machine learning and causality. At its core, my work is driven by the question: <strong><em>What is the right foundation for learning reliable representations of complex data?</em></strong> I develop the statistical and algorithmic foundations needed to answer this question, with emphasis on learning <strong>identifiable</strong> and <strong>causal</strong> mechanisms behind the data, particularly through <a href="projects/CRL2">Causal Representation Learning</a>[<a href="https://jmlr.org/papers/volume26/24-0194/24-0194.pdf" rel="external nofollow noopener" target="_blank">1</a>,<a href="https://proceedings.mlr.press/v238/varici24a/varici24a.pdf" rel="external nofollow noopener" target="_blank">2</a>,<a href="https://arxiv.org/abs/2406.05937" rel="external nofollow noopener" target="_blank">3</a>,<a href="https://openreview.net/pdf?id=XL9aaXl0u6" rel="external nofollow noopener" target="_blank">4</a>,<a href="https://arxiv.org/abs/2510.20884" rel="external nofollow noopener" target="_blank">5</a>]. I am especially keen on leveraging shared mechanisms across diverse environments to establish scalable and provably correct algorithms—a common theme in my broader research directions on <em>Causal Discovery</em>[<a href="https://arxiv.org/abs/2406.08666" rel="external nofollow noopener" target="_blank">6</a>,<a href="https://openreview.net/pdf?id=ALRWXT1RLZ" rel="external nofollow noopener" target="_blank">7</a>,<a href="https://arxiv.org/abs/2111.07512" rel="external nofollow noopener" target="_blank">8</a>,<a href="https://bvarici.github.io/assets/pdf/Varici2022-UAI.pdf">9</a>] and <em>Sequential Intervention Design</em>[<a href="https://jmlr.org/papers/volume24/22-0969/22-0969.pdf" rel="external nofollow noopener" target="_blank">10</a>,<a href="https://arxiv.org/pdf/2310.19794" rel="external nofollow noopener" target="_blank">11</a>] problems. I am also interested in increasing the efficiency and extent of <strong>identifiability</strong> in common representation learning paradigms[<a href="https://openreview.net/pdf?id=4GZwFPzLgW" rel="external nofollow noopener" target="_blank">12</a>,<a href="https://arxiv.org/abs/2510.24672" rel="external nofollow noopener" target="_blank">13</a>].</p> <blockquote> <p><strong>I’m currently on the academic job market!</strong> If you think I’d be a good fit to your department, please reach out!</p> </blockquote> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%62%76%61%72%69%63%69@%61%6E%64%72%65%77.%63%6D%75.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="/assets/pdf/Varici_CV.pdf" target="_blank" rel="noopener noreferrer" title="CV"><i class="ai ai-cv"></i></a> <a href="https://scholar.google.com/citations?user=v_SL5c4AAAAJ&amp;hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/bvarici" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/burak-var%C4%B1c%C4%B1-021067101/" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> </div> <div class="contact-note"> </div> </div> <h3><a href="/news/" style="color: inherit;">News</a></h3> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Nov 1, 2025</th> <td> Preprint: CRL application on robotics! <a href="https://arxiv.org/pdf/2510.20884" rel="external nofollow noopener" target="_blank">ROPES: Robotic Pose Estimation via Score-Based Causal Representation Learning</a>. It will appear at <a href="https://embodied-world-models.github.io/" rel="external nofollow noopener" target="_blank">NeurIPS Embodied World Models Workshop</a>, see you at San Diego! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 28, 2025</th> <td> New preprint on identifiable representation learning: <a href="https://arxiv.org/pdf/2510.24672" rel="external nofollow noopener" target="_blank">Eigenfunction Extraction for Ordered Representation Learning</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 13, 2025</th> <td> Our paper <a href="https://www.jmlr.org/papers/volume26/24-0194/24-0194.pdf" rel="external nofollow noopener" target="_blank">Score-based Causal Representation Learning: Linear and General Transformations</a> is published in Journal of Machine Learning Research (JMLR)! </td> </tr> <tr> <th scope="row" style="width: 20%">May 1, 2025</th> <td> Our paper <a href="https://arxiv.org/abs/2505.01557" rel="external nofollow noopener" target="_blank">Contextures: Representations from Contexts</a> is accepted to ICML. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 1, 2025</th> <td> Attending AAAI at Philadephia! Check out our tutorial on Causal Representation Learning, see the <a href="https://www.isg-rpi.com/aaai-25" rel="external nofollow noopener" target="_blank">slides here</a>. Also gave a talk at the workshop on <a href="https://sites.google.com/view/aict-2025/" rel="external nofollow noopener" target="_blank">AI with Causal Techniques</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 26, 2025</th> <td> Our paper <a href="https://openreview.net/forum?id=vkvJDRmOLs" rel="external nofollow noopener" target="_blank">On the Consistent Recovery of Joint Distributions from Conditionals</a> is accepted to AISTATS. </td> </tr> </table> </div> </div> <h3> <a href="/publications/" style="color: inherit;">Selected Publications</a> </h3> <p style="margin-top: -0.5rem; font-size: 0.9rem;"> See <a href="/publications/">publications</a> for the complete list. </p> <div class="publications selected-publications"> <h2 class="bibliography">1</h2> <ol class="bibliography"><li><div class="row" style="margin-bottom: 30px;"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JMLR</abbr> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/crl_jmlr_overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/crl_jmlr_overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/crl_jmlr_overview-1400.webp"></source> <img src="/assets/img/publication_preview/crl_jmlr_overview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="crl_jmlr_overview.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="varici2025score" class="col-sm-8"> <div class="title">Score-based causal representation learning: Linear and General Transformations</div> <div class="author"> Burak Varıcı*, Emre Acartürk*, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer </div> <div class="periodical"> <em>Journal of Machine Learning Research</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://jmlr.org/papers/volume26/24-0194/24-0194.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/acarturk-e/score-based-crl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the identifiability and achievability aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between score functions (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a score-based class of algorithms that ensures both identifiability and achievability. First, the paper focuses on linear transformations and shows that one stochastic hard intervention per node suffices to guarantee identifiability. It also provides partial identifiability guarantees for soft interventions, including identifiability up to ancestors for general causal models and perfect latent graph recovery for sufficiently non-linear causal models. Secondly, it focuses on general transformations and shows that two stochastic hard interventions per node suffice for identifiability. Notably, one does not need to know which pair of interventional environments have the same node intervened. Finally, the theoretical results are empirically validated via experiments on structured synthetic data and image data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">varici2025score</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Score-based causal representation learning: Linear and General Transformations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Var{\i}c{\i}*, Burak and Acart{\"u}rk*, Emre and Shanmugam, Karthikeyan and Kumar, Abhishek and Tajer, Ali}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{26}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{112}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--90}</span><span class="p">,</span>
  <span class="na">rank</span> <span class="p">=</span> <span class="s">{1}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li></ol> <h2 class="bibliography">2</h2> <ol class="bibliography"><li><div class="row" style="margin-bottom: 30px;"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JMLR</abbr> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/causal_bandits_thumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/causal_bandits_thumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/causal_bandits_thumbnail-1400.webp"></source> <img src="/assets/img/publication_preview/causal_bandits_thumbnail.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="causal_bandits_thumbnail.jpeg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="varici2023causal" class="col-sm-8"> <div class="title">Causal Bandits for Linear Structural Equation Models</div> <div class="author"> <em>Burak Varıcı</em>, Karthikeyan Shanmugam, Prasanna Sattigeri, and Ali Tajer </div> <div class="periodical"> <em>Journal of Machine Learning Research</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://jmlr.org/papers/volume24/22-0969/22-0969.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/bvarici/causal-bandits-linear-sem" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/JMLR_causal_bandits_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>This paper studies the problem of designing an optimal sequence of interventions in a causal graphical model to minimize cumulative regret with respect to the best intervention in hindsight. This is, naturally, posed as a causal bandit problem. The focus is on causal bandits for linear structural equation models (SEMs) and soft interventions. It is assumed that the graph’s structure is known and has \(N\)nodes. Two linear mechanisms, one soft intervention and one observational, are assumed for each node, giving rise to \(2^N\)possible interventions. The majority of the existing causal bandit algorithms assume that at least the interventional distributions of the reward node’s parents are fully specified. However, there are \(2^N\)such distributions (one corresponding to each intervention), acquiring which becomes prohibitive even in moderate-sized graphs. This paper dispenses with the assumption of knowing these distributions or their marginals. Two algorithms are proposed for the frequentist (UCB-based) and Bayesian (Thompson sampling-based) settings. The key idea of these algorithms is to avoid directly estimating the \(2^N\)reward distributions and instead estimate the parameters that fully specify the SEMs (linear in \(N\)) and use them to compute the rewards. In both algorithms, under boundedness assumptions on noise and the parameter space, the cumulative regrets scale as \(\tilde{O}(d^L+\frac12 \sqrt{NT})\), where \(d\)is the graph’s maximum degree, and \(L\)is the length of its longest causal path. Additionally, a minimax lower of \(Ω(d^{L/2-2}\sqrt{T})\)is presented, which suggests that the achievable and lower bounds conform in their scaling behavior with respect to the horizon \(T\)and graph parameters \(d\)and \(L\).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">varici2023causal</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Var{\i}c{\i}, Burak and Shanmugam, Karthikeyan and Sattigeri, Prasanna and Tajer, Ali}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Causal Bandits for Linear Structural Equation Models}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{297}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--59}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://jmlr.org/papers/v24/22-0969.html}</span><span class="p">,</span>
  <span class="na">video_short</span> <span class="p">=</span> <span class="s">{https://neurips.cc/virtual/2024/poster/98317}</span><span class="p">,</span>
  <span class="na">rank</span> <span class="p">=</span> <span class="s">{2}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li></ol> <h2 class="bibliography">3</h2> <ol class="bibliography"><li><div class="row" style="margin-bottom: 30px;"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/contextures_central_argument-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/contextures_central_argument-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/contextures_central_argument-1400.webp"></source> <img src="/assets/img/publication_preview/contextures_central_argument.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="contextures_central_argument.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhai2025contextures" class="col-sm-8"> <div class="title">Contextures: Representations from Contexts</div> <div class="author"> Runtian Zhai, Kai Yang, <em>Burak Varıcı</em>, Che-Ping Tsai, J. Zico Kolter, and Pradeep Ravikumar </div> <div class="periodical"> <em>In International Conference on Machine Learning</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=4GZwFPzLgW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://colab.research.google.com/drive/1GdJ0Yn-PKiKfkZIwUuon3WpTpbNWEtAO?usp=sharing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/Contextures-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>Despite the empirical success of foundation models, we do not have a systematic characterization of the representations that these models learn. In this paper, we establish the contexture theory. It shows that a large class of representation learning methods can be characterized as learning from the association between the input and a context variable. Specifically, we show that many popular methods aim to approximate the top-d singular functions of the expectation operator induced by the context, in which case we say that the representation learns the contexture. We demonstrate the generality of the contexture theory by proving that representation learning within various learning paradigms – supervised, self-supervised, and manifold learning – can all be studied from such a perspective. We also prove that the representations that learn the contexture are optimal on those tasks that are compatible with the context. One important implication of the contexture theory is that once the model is large enough to approximate the top singular functions, further scaling up the model size yields diminishing returns. Therefore, scaling is not all we need, and further improvement requires better contexts. To this end, we study how to evaluate the usefulness of a context without knowing the downstream tasks. We propose a metric and show by experiments that it correlates well with the actual performance of the encoder on many real datasets.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhai2025contextures</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Contextures: Representations from Contexts}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhai, Runtian and Yang, Kai and Var{\i}c{\i}, Burak and Tsai, Che-Ping and Kolter, J. Zico and Ravikumar, Pradeep}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">rank</span> <span class="p">=</span> <span class="s">{3}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li></ol> <h2 class="bibliography">4</h2> <ol class="bibliography"><li><div class="row" style="margin-bottom: 30px;"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/int_mixture_dag-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/int_mixture_dag-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/int_mixture_dag-1400.webp"></source> <img src="/assets/img/publication_preview/int_mixture_dag.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="int_mixture_dag.jpeg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="varici2024interventional" class="col-sm-8"> <div class="title">Interventional Causal Discovery in a Mixture of DAGs</div> <div class="author"> <em>Burak Varıcı</em>, Dmitriy A Katz, Dennis Wei, Prasanna Sattigeri, and Ali Tajer </div> <div class="periodical"> <em>In Proc. Advances in Neural Information Processing Systems</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2406.08666" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/bvarici/intervention-mixture-DAG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/Int_Mixture_DAG_poster_final.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://neurips.cc/media/neurips-2024/Slides/93767.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </div> <div class="abstract hidden"> <p>Causal interactions among a group of variables are often modeled by a single causal graph. In some domains, however, these interactions are best described by multiple co-existing causal graphs, e.g., in dynamical systems or genomics. This paper addresses the hitherto unknown role of interventions in learning causal interactions among variables governed by a mixture of causal systems, each modeled by one directed acyclic graph (DAG). Causal discovery from mixtures is fundamentally more challenging than single-DAG causal discovery. Two major difficulties stem from (i) an inherent uncertainty about the skeletons of the component DAGs that constitute the mixture and (ii) possibly cyclic relationships across these component DAGs. This paper addresses these challenges and aims to identify edges that exist in at least one component DAG of the mixture, referred to as the true edges. First, it establishes matching necessary and sufficient conditions on the size of interventions required to identify the true edges. Next, guided by the necessity results, an adaptive algorithm is designed that learns all true edges using \(O(n^2)\)interventions, where \(n\)is the number of nodes. Remarkably, the size of the interventions is optimal if the underlying mixture model does not contain cycles across its components. More generally, the gap between the intervention size used by the algorithm and the optimal size is quantified. It is shown to be bounded by the cyclic complexity number of the mixture model, defined as the size of the minimal intervention that can break the cycles in the mixture, which is upper bounded by the number of cycles among the ancestors of a node.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">varici2024interventional</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Interventional Causal Discovery in a Mixture of DAGs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Var{\i}c{\i}, Burak and Katz, Dmitriy A and Wei, Dennis and Sattigeri, Prasanna and Tajer, Ali}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proc. Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">video_short</span> <span class="p">=</span> <span class="s">{https://neurips.cc/virtual/2024/poster/93767}</span><span class="p">,</span>
  <span class="na">rank</span> <span class="p">=</span> <span class="s">{4}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li></ol> <h2 class="bibliography">5</h2> <ol class="bibliography"><li><div class="row" style="margin-bottom: 30px;"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AISTATS (oral)</abbr> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/aistats24_crl_thumbnail3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/aistats24_crl_thumbnail3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/aistats24_crl_thumbnail3-1400.webp"></source> <img src="/assets/img/publication_preview/aistats24_crl_thumbnail3.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="aistats24_crl_thumbnail3.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="varici2024general" class="col-sm-8"> <div class="title">General Identifiability and Achievability for Causal Representation Learning</div> <div class="author"> <em>Burak Varıcı</em>, Emre Acartürk, Karthikeyan Shanmugam, and Ali Tajer </div> <div class="periodical"> <em>In Proc. International Conference on Artificial Intelligence and Statistics</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.mlr.press/v238/varici24a/varici24a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/acarturk-e/score-based-crl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/AISTATS_CRL_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://virtual.aistats.org/media/aistats-2024/Slides/6689.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </div> <div class="abstract hidden"> <p>This paper focuses on causal representation learning (CRL) under a general nonparametric latent causal model and a general transformation model that maps the latent data to the observational data. It establishes identifiability and achievability results using two hard uncoupled interventions per node in the latent causal graph. Notably, one does not know which pair of intervention environments have the same node intervened (hence, uncoupled). For identifiability, the paper establishes that perfect recovery of the latent causal model and variables is guaranteed under uncoupled interventions. For achievability, an algorithm is designed that uses observational and interventional data and recovers the latent causal model and variables with provable guarantees. This algorithm leverages score variations across different environments to estimate the inverse of the transformer and, subsequently, the latent variables. The analysis, additionally, recovers the identifiability result for two hard coupled interventions, that is when metadata about the pair of environments that have the same node intervened is known. This paper also shows that when observational data is available, additional faithfulness assumptions that are adopted by the existing literature are unnecessary.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">varici2024general</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{General Identifiability and Achievability for Causal Representation Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Var{\i}c{\i}, Burak and Acart{\"u}rk, Emre and Shanmugam, Karthikeyan and Tajer, Ali}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proc. International Conference on Artificial Intelligence and Statistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Valencia, Spain}</span><span class="p">,</span>
  <span class="na">rank</span> <span class="p">=</span> <span class="s">{5}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Burak Varıcı. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>